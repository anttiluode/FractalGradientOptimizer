
# Fractal Optimizer

![Fractal Optimizer Results](Figure_1.png)

üåä **Fractal Gradient Coupling Optimizer**

This project implements a novel optimizer that injects fractal 1/f^Œ≤ noise into gradient descent steps. It is designed to explore whether continuous, scale-invariant stochasticity can outperform or enhance traditional optimizers like Adam.

## Features

- Fractal noise injection based on 1/f^Œ≤ distributions
- Works with regression and classification datasets
- CUDA acceleration support
- Comparison between Adam and Fractal optimizers

## Results Summary

| Task                   | Fractal Win? | Notes                                   |
|------------------------|--------------|-----------------------------------------|
| Large Classification   | ‚ùå           | Promising but underperforms Adam        |
| California Housing     | ‚ùå           | Close, but slightly below Adam          |
| Complex Regression     | ‚úÖ           | Outperforms Adam by ~2.7%               |

## Usage

```bash
python fractal_optimizer.py
```

Make sure you have `torch`, `scikit-learn`, and `numpy` installed.

## License

MIT License
